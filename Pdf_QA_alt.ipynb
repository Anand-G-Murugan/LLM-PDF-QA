{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ngunc1MwPI"
      },
      "source": [
        "# PDF QA Bot using OpenAI, FAISS, Langchain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I3s7IqmsNAGQ"
      },
      "source": [
        "* The program uses Langchain's text splitter to split the pdf into chunks of data.\n",
        "* These Chunks are embedded using an embedding model from Huggingface.\n",
        "* The vectors are then stored using FAISS.\n",
        "* We then take an input question from the user.\n",
        "* The program uses vector similarity search to find the most relevant chunk of the pdf to the user's question.\n",
        "* This chunk is sent to the LLM (OpenAI's GPT-3) along with the user's question.\n",
        "* The LLM then generates an appropriate answer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q streamlit PyPDF2 python-dotenv faiss-cpu langchain altair openai tiktoken sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Huggingface Embeddings\n",
        "# OpenAI LLM\n",
        "# FAISS Vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# get OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_KEY_HERE\"\n",
        "name = os.environ[\"OPENAI_API_KEY\"]\n",
        "if(name):\n",
        "  print(\"OpenAI key has been entered!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# upload file\n",
        "pdf_path = \"./path/to/pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract the text from the pdf\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "  text += page.extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split into chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "  separator=\"\\n\",\n",
        "  chunk_size=1000,\n",
        "  chunk_overlap=200,\n",
        "  length_function=len\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define embedding function\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Embeddings model importedfrom Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# selecting LLM\n",
        "llm = OpenAI() # by default -> GPT-3 davinci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knowledge_base = FAISS.from_texts(chunks, embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get user input\n",
        "user_question = \"YOUR_QUESTION_HERE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = knowledge_base.similarity_search(user_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "with get_openai_callback() as cb:\n",
        "  response = chain.run(input_documents=docs, question=user_question)\n",
        "  print(cb)\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
